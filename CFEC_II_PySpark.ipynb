{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Categorical Feature Encoding Challenge II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Kaggle competition is based on the artificially created dataset where every feature is categorical. The dataset that can be downloaded from\n",
    "\n",
    "https://www.kaggle.com/c/cat-in-the-dat-ii/data\n",
    "\n",
    "consists of 5 binary features, 10 low- and high-cardinality nominal features, 6 low- and high-cardinality ordinal features and 2 potentially cyclic features. This is a Playground competition that allows participants to build machine learning skills by trying different encoding schemes, trying different ways to impute missing values and comparing performance of different algorithms. The target variable is an ordinary binary variable.\n",
    "\n",
    "The purpose of the project presented in this notebook, is to do predictive modelling based on the\n",
    "methods of PySpark (rather than scikit-learn) machine learning library. While using PySpark is hardly justified for relatively small datasets (the sizes of train and test sets are 83MB and 55MB respectively), it is interesting to see how accurate predictions can be if one uses built-in classifiers from pyspark.ml module.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and inspecting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#create the spark session\n",
    "spark = SparkSession.builder.appName('CFEC_II').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the garbage collection module\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.76953125"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the module that shows the memory usage\n",
    "\n",
    "import os, psutil\n",
    "\n",
    "def usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info()[0] / float(2 ** 20)\n",
    "    \n",
    "usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- bin_0: double (nullable = true)\n",
      " |-- bin_1: double (nullable = true)\n",
      " |-- bin_2: double (nullable = true)\n",
      " |-- bin_3: string (nullable = true)\n",
      " |-- bin_4: string (nullable = true)\n",
      " |-- nom_0: string (nullable = true)\n",
      " |-- nom_1: string (nullable = true)\n",
      " |-- nom_2: string (nullable = true)\n",
      " |-- nom_3: string (nullable = true)\n",
      " |-- nom_4: string (nullable = true)\n",
      " |-- nom_5: string (nullable = true)\n",
      " |-- nom_6: string (nullable = true)\n",
      " |-- nom_7: string (nullable = true)\n",
      " |-- nom_8: string (nullable = true)\n",
      " |-- nom_9: string (nullable = true)\n",
      " |-- ord_0: double (nullable = true)\n",
      " |-- ord_1: string (nullable = true)\n",
      " |-- ord_2: string (nullable = true)\n",
      " |-- ord_3: string (nullable = true)\n",
      " |-- ord_4: string (nullable = true)\n",
      " |-- ord_5: string (nullable = true)\n",
      " |-- day: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = spark.read.csv('../data/train.csv', header = True, inferSchema = True)\n",
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- bin_0: double (nullable = true)\n",
      " |-- bin_1: double (nullable = true)\n",
      " |-- bin_2: double (nullable = true)\n",
      " |-- bin_3: string (nullable = true)\n",
      " |-- bin_4: string (nullable = true)\n",
      " |-- nom_0: string (nullable = true)\n",
      " |-- nom_1: string (nullable = true)\n",
      " |-- nom_2: string (nullable = true)\n",
      " |-- nom_3: string (nullable = true)\n",
      " |-- nom_4: string (nullable = true)\n",
      " |-- nom_5: string (nullable = true)\n",
      " |-- nom_6: string (nullable = true)\n",
      " |-- nom_7: string (nullable = true)\n",
      " |-- nom_8: string (nullable = true)\n",
      " |-- nom_9: string (nullable = true)\n",
      " |-- ord_0: double (nullable = true)\n",
      " |-- ord_1: string (nullable = true)\n",
      " |-- ord_2: string (nullable = true)\n",
      " |-- ord_3: string (nullable = true)\n",
      " |-- ord_4: string (nullable = true)\n",
      " |-- ord_5: string (nullable = true)\n",
      " |-- day: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = spark.read.csv('../data/test.csv', header = True, inferSchema = True)\n",
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+-----+-----+---------+-------+----------+--------+---------+---------+---------+---------+---------+-----+-----------+-----------+-----+-----+-----+---+-----+------+\n",
      "| id|bin_0|bin_1|bin_2|bin_3|bin_4|nom_0|    nom_1|  nom_2|     nom_3|   nom_4|    nom_5|    nom_6|    nom_7|    nom_8|    nom_9|ord_0|      ord_1|      ord_2|ord_3|ord_4|ord_5|day|month|target|\n",
      "+---+-----+-----+-----+-----+-----+-----+---------+-------+----------+--------+---------+---------+---------+---------+---------+-----+-----------+-----------+-----+-----+-----+---+-----+------+\n",
      "|  0|  0.0|  0.0|  0.0|    F|    N|  Red|Trapezoid|Hamster|    Russia| Bassoon|de4c57ee2|a64bc7ddf|598080a91|0256c7a4b|02e7c8990|  3.0|Contributor|        Hot|    c|    U|   Pw|6.0|  3.0|     0|\n",
      "|  1|  1.0|  1.0|  0.0|    F|    Y|  Red|     Star|Axolotl|      null|Theremin|2bb3c3e5c|3a3a936e8|1dddb8473|52ead350c|f37df64af|  3.0|Grandmaster|       Warm|    e|    X|   pE|7.0|  7.0|     0|\n",
      "|  2|  0.0|  1.0|  0.0|    F|    N|  Red|     null|Hamster|    Canada| Bassoon|b574c9841|708248125|5ddc9a726|745b909d1|     null|  3.0|       null|   Freezing|    n|    P|   eN|5.0|  9.0|     0|\n",
      "|  3| null|  0.0|  0.0|    F|    N|  Red|   Circle|Hamster|   Finland|Theremin|673bdf1f6|23edb8da3|3a33ef960|bdaa56dd1|f9d456e57|  1.0|     Novice|   Lava Hot|    a|    C| null|3.0|  3.0|     0|\n",
      "|  4|  0.0| null|  0.0|    T|    N|  Red| Triangle|Hamster|Costa Rica|    null|777d1ac2c|3a7975e46|bc9cc2a94|     null|c5361037c|  3.0|Grandmaster|       Cold|    h|    C|   OZ|5.0| 12.0|     0|\n",
      "|  5|  0.0| null|  1.0|    T|    N|  Red| Triangle|   Lion|     China| Bassoon|a2e1bf0b1|ae6737c29|8c30b9b0b|690411ac0|05afc0f8b|  2.0|     Expert|        Hot|    b|    Q|   wa|3.0|  4.0|     0|\n",
      "|  6|  0.0|  0.0|  0.0|    F|    N|  Red| Triangle|Hamster|Costa Rica| Bassoon|87a5be0d7|cdc35bd00|1cba571fa|b8e63cace|4d3766412|  1.0|Grandmaster|       Cold|    c|    R|   rg|5.0|  6.0|     0|\n",
      "|  7|  0.0|  0.0|  1.0|    T|    N|  Red| Triangle|Axolotl|   Finland| Bassoon|104aee31d|2a50808ba|81d67e1bb|bd9643a20|a651dec43|  3.0|     Expert|       Cold|    b|    Y|   PS|1.0|  1.0|     0|\n",
      "|  8|  0.0|  0.0|  0.0|    F|    N| Blue|  Polygon|Hamster|    Russia|    Oboe|024efa364|a4a81ab45|429114096|94c5fd40c|     null|  1.0|     Novice|Boiling Hot|    c|    N|   mX|6.0|  3.0|     0|\n",
      "|  9|  0.0|  0.0| null|    F|    Y|  Red|  Polygon|Hamster|   Finland|Theremin|9fa084b36|e7aa94f40|56d35c774|0279391c5|79b29d54c|  3.0|Contributor|   Lava Hot|    n|    I|   OZ|1.0|  8.0|     1|\n",
      "+---+-----+-----+-----+-----+-----+-----+---------+-------+----------+--------+---------+---------+---------+---------+---------+-----+-----------+-----------+-----+-----+-----+---+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-----+-----+-----+-----+---------+-------+----------+--------+---------+---------+---------+---------+---------+-----+-----------+-----------+-----+-----+-----+---+-----+\n",
      "|    id|bin_0|bin_1|bin_2|bin_3|bin_4|nom_0|    nom_1|  nom_2|     nom_3|   nom_4|    nom_5|    nom_6|    nom_7|    nom_8|    nom_9|ord_0|      ord_1|      ord_2|ord_3|ord_4|ord_5|day|month|\n",
      "+------+-----+-----+-----+-----+-----+-----+---------+-------+----------+--------+---------+---------+---------+---------+---------+-----+-----------+-----------+-----+-----+-----+---+-----+\n",
      "|600000|  0.0|  0.0|  0.0|    F|    Y| Blue|  Polygon|Axolotl|   Finland|   Piano|52f6dd16c|147d704e4|8d857a0a1|ca9ad1d4b|fced9e114|  3.0|     Novice|Boiling Hot|    f|    U|   oU|3.0|  9.0|\n",
      "|600001|  0.0|  0.0|  0.0|    F|    Y|  Red|   Circle|   Lion|    Russia| Bassoon|691ebeae8|8653dcc2e|67a8d4ebb|060a21580|7ca8775da|  1.0|     Novice|       Cold|    n|    N| null|2.0|  8.0|\n",
      "|600002|  0.0|  0.0|  0.0|    F|    Y| Blue|   Circle|Axolotl|    Russia|Theremin|81f792c16|6cdda499e|69403e18c|165e81a00|5940334c9|  1.0|     Expert|       Warm|    i|    N|   DN|2.0|  6.0|\n",
      "|600003|  1.0|  0.0|  0.0|    F|    N|  Red|  Polygon|Axolotl|Costa Rica| Bassoon|c9134205b|acbca4827|cb681246b|77d41330d|6fbdeefc8|  1.0|     Expert|        Hot|    m|    B|   AG|1.0|  6.0|\n",
      "|600004|  0.0|  0.0|  1.0|    F|    Y|  Red|   Circle|   null|   Finland|Theremin|f0f100f57|6f800b9af|cd9feb5c6|2218d9dfe|2a27c8fde|  1.0|Contributor|   Lava Hot|    o|    J|   DT|3.0|  3.0|\n",
      "|600005|  0.0|  0.0|  0.0| null|    Y|  Red|     Star|Axolotl|Costa Rica|Theremin|ce3da7e28|f8e870910|598080a91|745b909d1|30db15878|  3.0|Grandmaster|       Cold|    e|    Y|   th|5.0|  6.0|\n",
      "|600006|  0.0|  0.0|  0.0|    F|    Y|  Red|Trapezoid|Axolotl|    Russia| Bassoon|fb772f7e1|08dd65794|a7059911d|75a957c6d|ecbea3fb9|  2.0|     Novice|   Lava Hot|    n|    U|   RD|1.0|  6.0|\n",
      "|600007|  0.0|  0.0|  1.0|    F|    N|  Red|Trapezoid|Axolotl|     India|    Oboe|edd08fbe3|5473c547f|8c9b654fe|5bdcf7324|48ca7cb85|  1.0|Grandmaster|Boiling Hot|    f| null|   pT|2.0|  6.0|\n",
      "|600008|  0.0|  0.0|  0.0|    T|    Y|  Red| Triangle|    Dog|    Russia| Bassoon|c0192e379|88794687e|3599d0255|8aeac2495|e663c4eec|  2.0|     Master|       Warm|    c|    Y|   nj|5.0|  5.0|\n",
      "|600009|  0.0|  0.0|  0.0|    F|    N|Green|  Polygon|   Lion|Costa Rica| Bassoon|b2d1ecfad|0f59828ee|3ef3018d3|220190c9e|0ce4b3b6e|  2.0|     Novice|        Hot|    k|    M|   dp|7.0|  2.0|\n",
      "+------+-----+-----+-----+-----+-----+-----+---------+-------+----------+--------+---------+---------+---------+---------+---------+-----+-----------+-----------+-----+-----+-----+---+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that almost every row in both train_df and test_df contains a missing value. Let's count how many values are missing. 'id' and 'target' columns are the only ones that don't have missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+\n",
      "| id|bin_0|bin_1|bin_2|bin_3|bin_4|nom_0|nom_1|nom_2|nom_3|nom_4|nom_5|nom_6|nom_7|nom_8|nom_9|ord_0|ord_1|ord_2|ord_3|ord_4|ord_5|  day|month|target|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+\n",
      "|  0|17894|18003|17930|18014|18047|18252|18156|18035|18121|18035|17778|18131|18003|17755|18073|18288|18041|18075|17916|17930|17713|17952|17988|     0|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in train_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| id|bin_0|bin_1|bin_2|bin_3|bin_4|nom_0|nom_1|nom_2|nom_3|nom_4|nom_5|nom_6|nom_7|nom_8|nom_9|ord_0|ord_1|ord_2|ord_3|ord_4|ord_5|  day|month|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  0|11901|12038|11972|11951|11951|12062|11947|12179|12176|11993|11912|12012|12003|11956|12060|11893|12167|12105|12053|11933|12047|12025|11984|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in test_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see from below that both dataframes contain the categorical features (nominal and ordinal) whose cardinality varies significantly. Some columns of the  train set contain unique values not present in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+---+-----+------+\n",
      "|    id|bin_0|bin_1|bin_2|bin_3|bin_4|nom_0|nom_1|nom_2|nom_3|nom_4|nom_5|nom_6|nom_7|nom_8|nom_9|ord_0|ord_1|ord_2|ord_3|ord_4|ord_5|day|month|target|\n",
      "+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+---+-----+------+\n",
      "|600000|    2|    2|    2|    2|    2|    3|    6|    6|    6|    4| 1220| 1519|  222|  222| 2218|    3|    5|    6|   15|   26|  190|  7|   12|     2|\n",
      "+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select([F.countDistinct(F.col(c)).alias(c) for c in train_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+---+-----+\n",
      "|    id|bin_0|bin_1|bin_2|bin_3|bin_4|nom_0|nom_1|nom_2|nom_3|nom_4|nom_5|nom_6|nom_7|nom_8|nom_9|ord_0|ord_1|ord_2|ord_3|ord_4|ord_5|day|month|\n",
      "+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+---+-----+\n",
      "|400000|    2|    2|    2|    2|    2|    3|    6|    6|    6|    4| 1219| 1517|  222|  222| 2216|    3|    5|    6|   15|   26|  190|  7|   12|\n",
      "+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select([F.countDistinct(F.col(c)).alias(c) for c in test_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look closer what the 'nom_5', 'nom_6','nom_7','nom_8','nom_9' columns contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+---------+\n",
      "|    nom_5|    nom_6|    nom_7|    nom_8|    nom_9|\n",
      "+---------+---------+---------+---------+---------+\n",
      "|de4c57ee2|a64bc7ddf|598080a91|0256c7a4b|02e7c8990|\n",
      "|2bb3c3e5c|3a3a936e8|1dddb8473|52ead350c|f37df64af|\n",
      "|b574c9841|708248125|5ddc9a726|745b909d1|     null|\n",
      "|673bdf1f6|23edb8da3|3a33ef960|bdaa56dd1|f9d456e57|\n",
      "|777d1ac2c|3a7975e46|bc9cc2a94|     null|c5361037c|\n",
      "+---------+---------+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select('nom_5', 'nom_6','nom_7','nom_8','nom_9').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+---------+\n",
      "|    nom_5|    nom_6|    nom_7|    nom_8|    nom_9|\n",
      "+---------+---------+---------+---------+---------+\n",
      "|52f6dd16c|147d704e4|8d857a0a1|ca9ad1d4b|fced9e114|\n",
      "|691ebeae8|8653dcc2e|67a8d4ebb|060a21580|7ca8775da|\n",
      "|81f792c16|6cdda499e|69403e18c|165e81a00|5940334c9|\n",
      "|c9134205b|acbca4827|cb681246b|77d41330d|6fbdeefc8|\n",
      "|f0f100f57|6f800b9af|cd9feb5c6|2218d9dfe|2a27c8fde|\n",
      "+---------+---------+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select('nom_5', 'nom_6','nom_7','nom_8','nom_9').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the above columns contain different combinations of letters and numbers of various lengths. It is interesting to see that all these entries are different for all 5 columns 'nom_5', 'nom_6','nom_7','nom_8','nom_9'. All what they have in common is the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "print( train_df.select('nom_5').distinct().\\\n",
    "       join(train_df.select('nom_6').distinct(), \n",
    "       train_df.nom_5 == train_df.nom_6, how = 'inner').count(),\n",
    "      \n",
    "       train_df.select('nom_6').distinct().\\\n",
    "       join(train_df.select('nom_7').distinct(), \n",
    "       train_df.nom_6 == train_df.nom_7, how = 'inner').count(),\n",
    "      \n",
    "       train_df.select('nom_7').distinct().\\\n",
    "       join(train_df.select('nom_8').distinct(), \n",
    "       train_df.nom_7 == train_df.nom_8, how = 'inner').count(),\n",
    "      \n",
    "       train_df.select('nom_8').distinct().\\\n",
    "       join(train_df.select('nom_9').distinct(), \n",
    "       train_df.nom_8 == train_df.nom_9, how = 'inner').count() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of distinct values for a number of relatively low-cardinality features. Number of missing values is provided as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in the column bin_0 are (train set) dict_values([{nan: 17894, 1.0: 53729, 0.0: 528377}])\n",
      "Values in the column bin_1 are (train set) dict_values([{nan: 18003, 1.0: 107979, 0.0: 474018}])\n",
      "Values in the column bin_2 are (train set) dict_values([{nan: 17930, 1.0: 162225, 0.0: 419845}])\n",
      "Values in the column bin_3 are (train set) dict_values([{None: 18014, 'T': 215774, 'F': 366212}])\n",
      "Values in the column bin_4 are (train set) dict_values([{None: 18047, 'Y': 269609, 'N': 312344}])\n"
     ]
    }
   ],
   "source": [
    "for col in [col for col in train_df.columns if 'bin' in col]:\n",
    "\n",
    "    print( f'Values in the column {col} are (train set)', \n",
    "    train_df.groupBy(col).count().orderBy('count').toPandas().set_index(col).to_dict().values() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in the column bin_0 are (test set) dict_values([{nan: 11901, 1.0: 36322, 0.0: 351777}])\n",
      "Values in the column bin_1 are (test set) dict_values([{nan: 12038, 1.0: 72609, 0.0: 315353}])\n",
      "Values in the column bin_2 are (test set) dict_values([{nan: 11972, 1.0: 108030, 0.0: 279998}])\n",
      "Values in the column bin_3 are (test set) dict_values([{None: 11951, 'T': 143957, 'F': 244092}])\n",
      "Values in the column bin_4 are (test set) dict_values([{None: 11951, 'Y': 179622, 'N': 208427}])\n"
     ]
    }
   ],
   "source": [
    "for col in [col for col in test_df.columns if 'bin' in col]:\n",
    "\n",
    "    print( f'Values in the column {col} are (test set)', \n",
    "    test_df.groupBy(col).count().orderBy('count').toPandas().set_index(col).to_dict().values() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in the column nom_0 are (train set) dict_values([{None: 18252, 'Green': 52601, 'Blue': 205861, 'Red': 323286}])\n",
      "Values in the column nom_1 are (train set) dict_values([{'Star': 14155, None: 18156, 'Square': 26503, 'Circle': 104995, 'Trapezoid': 119438, 'Polygon': 152563, 'Triangle': 164190}])\n",
      "Values in the column nom_2 are (train set) dict_values([{'Snake': 14144, None: 18035, 'Cat': 26276, 'Dog': 104825, 'Lion': 119504, 'Axolotl': 152319, 'Hamster': 164897}])\n",
      "Values in the column nom_3 are (train set) dict_values([{'China': 14317, None: 18121, 'Canada': 26425, 'Finland': 104601, 'Russia': 119840, 'Costa Rica': 151827, 'India': 164869}])\n",
      "Values in the column nom_4 are (train set) dict_values([{None: 18035, 'Piano': 26709, 'Oboe': 49996, 'Bassoon': 196639, 'Theremin': 308621}])\n"
     ]
    }
   ],
   "source": [
    "for col in ['nom_0','nom_1', 'nom_2','nom_3','nom_4']:\n",
    "\n",
    "    print( f'Values in the column {col} are (train set)', \n",
    "    train_df.groupBy(col).count().orderBy('count').toPandas().set_index(col).to_dict().values() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in the column nom_0 are (test set) dict_values([{None: 12062, 'Green': 34894, 'Blue': 136592, 'Red': 216452}])\n",
      "Values in the column nom_1 are (test set) dict_values([{'Star': 9523, None: 11947, 'Square': 17398, 'Circle': 70076, 'Trapezoid': 80025, 'Polygon': 101389, 'Triangle': 109642}])\n",
      "Values in the column nom_2 are (test set) dict_values([{'Snake': 9416, None: 12179, 'Cat': 17641, 'Dog': 69927, 'Lion': 79702, 'Axolotl': 101836, 'Hamster': 109299}])\n",
      "Values in the column nom_3 are (test set) dict_values([{'China': 9401, None: 12176, 'Canada': 17619, 'Finland': 69587, 'Russia': 80093, 'Costa Rica': 101447, 'India': 109677}])\n",
      "Values in the column nom_4 are (test set) dict_values([{None: 11993, 'Piano': 17673, 'Oboe': 33332, 'Bassoon': 131465, 'Theremin': 205537}])\n"
     ]
    }
   ],
   "source": [
    "for col in ['nom_0','nom_1', 'nom_2','nom_3','nom_4']:\n",
    "\n",
    "    print( f'Values in the column {col} are (test set)', \n",
    "    test_df.groupBy(col).count().orderBy('count').toPandas().set_index(col).to_dict().values() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in the column ord_0 are (train set) dict_values([{nan: 18288, 2.0: 155997, 3.0: 197798, 1.0: 227917}])\n",
      "Values in the column ord_1 are (train set) dict_values([{None: 18041, 'Master': 75998, 'Grandmaster': 95866, 'Contributor': 109821, 'Expert': 139677, 'Novice': 160597}])\n",
      "Values in the column ord_2 are (train set) dict_values([{None: 18075, 'Lava Hot': 64840, 'Hot': 67508, 'Boiling Hot': 84790, 'Cold': 97822, 'Warm': 124239, 'Freezing': 142726}])\n",
      "Values in the column ord_3 are (train set) dict_values([{'l': 2835, 'j': 3639, 'g': 6180, None: 17916, 'f': 29450, 'd': 30634, 'i': 34763, 'k': 38718, 'e': 38904, 'b': 44795, 'o': 45464, 'h': 55744, 'c': 56675, 'm': 57980, 'a': 65321, 'n': 70982}])\n"
     ]
    }
   ],
   "source": [
    "for col in ['ord_0', 'ord_1','ord_2','ord_3']:\n",
    "\n",
    "    print( f'Values in the column {col} are (train set)', \n",
    "    train_df.groupBy(col).count().orderBy('count').toPandas().set_index(col).to_dict().values() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in the column ord_0 are (test set) dict_values([{nan: 11893, 2.0: 104146, 3.0: 132302, 1.0: 151659}])\n",
      "Values in the column ord_1 are (test set) dict_values([{None: 12167, 'Master': 51086, 'Grandmaster': 63986, 'Contributor': 73069, 'Expert': 92863, 'Novice': 106829}])\n",
      "Values in the column ord_2 are (test set) dict_values([{None: 12105, 'Lava Hot': 43493, 'Hot': 44509, 'Boiling Hot': 56624, 'Cold': 65042, 'Warm': 82940, 'Freezing': 95287}])\n",
      "Values in the column ord_3 are (test set) dict_values([{'l': 1957, 'j': 2452, 'g': 4203, None: 12053, 'f': 19771, 'd': 20552, 'i': 23453, 'k': 25600, 'e': 25628, 'b': 29456, 'o': 30484, 'h': 36974, 'c': 37888, 'm': 38372, 'a': 43625, 'n': 47532}])\n"
     ]
    }
   ],
   "source": [
    "for col in ['ord_0', 'ord_1','ord_2','ord_3']:\n",
    "\n",
    "    print( f'Values in the column {col} are (test set)', \n",
    "    test_df.groupBy(col).count().orderBy('count').toPandas().set_index(col).to_dict().values() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we count distinct values in the target feature of train_df. We see that our dataset is rather imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in the target column are (train set) dict_values([{1: 112323, 0: 487677}])\n"
     ]
    }
   ],
   "source": [
    "print( f'Values in the target column are (train set)', \n",
    "          train_df.groupBy('target').count().orderBy('count').\\\n",
    "      toPandas().set_index('target').to_dict().values() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check that columns 'ord_4' and 'ord_5' in train_df and test_df dataframes \n",
    "contain the same set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_df.select('ord_4').distinct().toPandas().ord_4) ==\\\n",
    "set(test_df.select('ord_4').distinct().toPandas().ord_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_df.select('ord_5').distinct().toPandas().ord_5) ==\\\n",
    "set(test_df.select('ord_5').distinct().toPandas().ord_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will first use the Pipeline to encode features. The process includes category indexing and subsequent assembling encoded features. Missing values are taken care of automatically during the procedure of category encoding. After the major preprocessing steps, the Gradient Boosting Tree and Random Forest Classifiers are used to fit the training data via 3-fold cross-validation. Finally, the GBT classifier is used to make predictions for the target variable on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low cardinality columns\n",
    "cols1 = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1', 'nom_2', \n",
    "         'nom_3', 'nom_4', 'ord_0', 'ord_1', 'ord_2']\n",
    "\n",
    "#high cardinality columns\n",
    "cols2 = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_3', 'ord_4', 'ord_5', 'day', 'month']\n",
    "\n",
    "cols = cols1 + cols2\n",
    "\n",
    "#stages for the pipeline\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treat one column at a time\n",
    "for col in cols:\n",
    "    strInd = StringIndexer(inputCol = col, outputCol = col + '_idx', handleInvalid=\"keep\")\n",
    "    \n",
    "    stages += [strInd]\n",
    "\n",
    "\n",
    "#do the vector assembling\n",
    "assemblerInputs = [col + '_idx' for col in cols] \n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the classification algorithms, we must perform the final preparatory steps described above for both train_df and test_df using the pipeline. We also extract 'id' column from the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract id's from the test data\n",
    "ids = test_df.select(\"id\").rdd.map(lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      " |-- bin_0: double (nullable = true)\n",
      " |-- bin_1: double (nullable = true)\n",
      " |-- bin_2: double (nullable = true)\n",
      " |-- bin_3: string (nullable = true)\n",
      " |-- bin_4: string (nullable = true)\n",
      " |-- nom_0: string (nullable = true)\n",
      " |-- nom_1: string (nullable = true)\n",
      " |-- nom_2: string (nullable = true)\n",
      " |-- nom_3: string (nullable = true)\n",
      " |-- nom_4: string (nullable = true)\n",
      " |-- ord_0: double (nullable = true)\n",
      " |-- ord_1: string (nullable = true)\n",
      " |-- ord_2: string (nullable = true)\n",
      " |-- nom_5: string (nullable = true)\n",
      " |-- nom_6: string (nullable = true)\n",
      " |-- nom_7: string (nullable = true)\n",
      " |-- nom_8: string (nullable = true)\n",
      " |-- nom_9: string (nullable = true)\n",
      " |-- ord_3: string (nullable = true)\n",
      " |-- ord_4: string (nullable = true)\n",
      " |-- ord_5: string (nullable = true)\n",
      " |-- day: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      "\n",
      "+------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                    |\n",
      "+------------------------------------------------------------------------------------------------------------+\n",
      "|[0.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,2.0,1.0,1.0,2.0,4.0,706.0,1288.0,146.0,159.0,1358.0,3.0,5.0,42.0,2.0,1.0]  |\n",
      "|[1.0,1.0,0.0,0.0,1.0,0.0,5.0,1.0,6.0,0.0,1.0,3.0,1.0,264.0,648.0,11.0,99.0,25.0,7.0,7.0,57.0,3.0,5.0]       |\n",
      "|[0.0,1.0,0.0,0.0,0.0,0.0,6.0,0.0,4.0,1.0,1.0,5.0,0.0,292.0,158.0,81.0,32.0,2218.0,0.0,1.0,99.0,1.0,9.0]     |\n",
      "|(23,[0,6,8,12,13,14,15,16,17,18,19,20,22],[2.0,3.0,3.0,5.0,450.0,324.0,91.0,127.0,1597.0,1.0,8.0,190.0,1.0])|\n",
      "|[0.0,2.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,4.0,1.0,3.0,2.0,153.0,1040.0,28.0,222.0,2.0,4.0,8.0,43.0,1.0,3.0]      |\n",
      "+------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "\n",
    "pipelineModel = pipeline.fit(train_df)\n",
    "train_df = pipelineModel.transform(train_df)\n",
    "\n",
    "selectedCols = ['features','target'] + cols\n",
    "train_df = train_df.select(selectedCols)\n",
    "\n",
    "train_df.printSchema()\n",
    "train_df.select('features').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = stages)\n",
    "\n",
    "pipelineModel = pipeline.fit(test_df)\n",
    "test_df = pipelineModel.transform(test_df)\n",
    "\n",
    "selectedCols = ['features'] + cols\n",
    "test_df = test_df.select(selectedCols)\n",
    "\n",
    "#test_df.printSchema()\n",
    "#train_df.select('features').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Tree Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use the Gradient Boosting Tree Classifier to train the model. We will do the GridSearch 3-fold cross-validation, choosing the best model among the models with maxIter =20, and stepSize equal to 0.1, 0.15 and 0.2. Note that parameter maxBin must be set to 2219, the number of categories in the column with the highest cardinality. Evaluator will evaluate the area under ROC curve. The seed parameter is 789."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initialize the GBT classifier\n",
    "gbtcl = GBTClassifier(labelCol=\"target\", featuresCol=\"features\", \n",
    "                      predictionCol='prediction', maxDepth=5, maxIter=20, maxBins = 2219,\n",
    "                      seed = 789)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol = 'target')\n",
    "\n",
    "# Create a parameter grid builder\n",
    "params = ParamGridBuilder()\n",
    "\n",
    "# Add grid points\n",
    "params = params.addGrid(gbtcl.stepSize, [0.1, 0.15, 0.2])\n",
    "# Construct the grid\n",
    "params = params.build()\n",
    "\n",
    "\n",
    "# 3-fold cross validation\n",
    "cv = CrossValidator(estimator=gbtcl, estimatorParamMaps=params, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "#fit the model on train data\n",
    "gbtModel = cv.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the best model is the model with stepSize parameter equal to 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stepSize: Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default: 0.1, current: 0.2)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbtModel.bestModel.explainParam('stepSize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area under the ROC curve obtained as a result of cross-validation is 0.6782."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC curve is: 0.6782561292772246\n"
     ]
    }
   ],
   "source": [
    "# display CV score\n",
    "auc_roc = gbtModel.avgMetrics[0]\n",
    "print('The area under ROC curve is:', auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the Random Forest Classifier for training. We again do the GridSearch 3-fold cross-validation, choosing the best model among the models with maxDepth =5, and numTrees equal to 20, 50 and 100. The same evaluator, area under ROC curve, is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initialize the Random Forest Classifier\n",
    "rfcl = RandomForestClassifier(labelCol=\"target\", featuresCol=\"features\", \n",
    "                    predictionCol='prediction', maxDepth=5, subsamplingRate=1.0,\n",
    "                    maxBins = 2219, seed = 789)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol = 'target')\n",
    "\n",
    "# Create a parameter grid builder\n",
    "params = ParamGridBuilder()\n",
    "\n",
    "# Add grid points\n",
    "params = params.addGrid(rfcl.numTrees, [20, 50, 80])\n",
    "# Construct the grid\n",
    "params = params.build()\n",
    "\n",
    "\n",
    "# 3-fold cross validation\n",
    "cv = CrossValidator(estimator=rfcl, estimatorParamMaps=params, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "#fit the model on train data\n",
    "rfModel = cv.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is the model with numTrees parameter equal to 80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'numTrees: Number of trees to train (>= 1) (default: 20, current: 80)'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfModel.bestModel.explainParam('numTrees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area under the ROC curve is only 0.6324."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC curve is: 0.6324591567908788\n"
     ]
    }
   ],
   "source": [
    "# display CV score\n",
    "auc_roc = rfModel.avgMetrics[0]\n",
    "print('The area under ROC curve is:', auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the GBT classifier tends to give better results, we will use it to make predictions on the test data set. For submission, we must extract the probability of the second class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gbtModel.transform(test_df)\n",
    "\n",
    "# extract the probability of the second class\n",
    "targets = predictions.select(\"probability\").rdd.map(lambda x: float(x[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare submission, we create numpy arrays out of ids and targets, create the corresponding Pandas dataframe and write if to csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 400000\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "\n",
    "#create numpy arrays\n",
    "ids = array(ids.collect())\n",
    "targets = array(targets.collect())\n",
    "\n",
    "print(len(ids), len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "subm = DataFrame({'id': ids, 'target': targets })\n",
    "                    \n",
    "subm.to_csv('submission3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will look at the table with the relative feature importances obtained as a result of training. We see that the high cardinality nominal features play the major role in classification decisions. Feature importances table is useful while making the actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nom_6_idx</td>\n",
       "      <td>0.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nom_9_idx</td>\n",
       "      <td>0.258527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nom_5_idx</td>\n",
       "      <td>0.228987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ord_3_idx</td>\n",
       "      <td>0.045862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nom_8_idx</td>\n",
       "      <td>0.027048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nom_7_idx</td>\n",
       "      <td>0.026544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ord_0_idx</td>\n",
       "      <td>0.025208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>month_idx</td>\n",
       "      <td>0.025045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ord_5_idx</td>\n",
       "      <td>0.023894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ord_2_idx</td>\n",
       "      <td>0.017023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ord_4_idx</td>\n",
       "      <td>0.015014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nom_1_idx</td>\n",
       "      <td>0.010236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bin_0_idx</td>\n",
       "      <td>0.008680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bin_2_idx</td>\n",
       "      <td>0.008518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ord_1_idx</td>\n",
       "      <td>0.006737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nom_3_idx</td>\n",
       "      <td>0.005521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>day_idx</td>\n",
       "      <td>0.004598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>nom_2_idx</td>\n",
       "      <td>0.002458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bin_1_idx</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bin_3_idx</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bin_4_idx</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>nom_0_idx</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>nom_4_idx</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature    weight\n",
       "0   nom_6_idx  0.260100\n",
       "1   nom_9_idx  0.258527\n",
       "2   nom_5_idx  0.228987\n",
       "3   ord_3_idx  0.045862\n",
       "4   nom_8_idx  0.027048\n",
       "5   nom_7_idx  0.026544\n",
       "6   ord_0_idx  0.025208\n",
       "7   month_idx  0.025045\n",
       "8   ord_5_idx  0.023894\n",
       "9   ord_2_idx  0.017023\n",
       "10  ord_4_idx  0.015014\n",
       "11  nom_1_idx  0.010236\n",
       "12  bin_0_idx  0.008680\n",
       "13  bin_2_idx  0.008518\n",
       "14  ord_1_idx  0.006737\n",
       "15  nom_3_idx  0.005521\n",
       "16    day_idx  0.004598\n",
       "17  nom_2_idx  0.002458\n",
       "18  bin_1_idx  0.000000\n",
       "19  bin_3_idx  0.000000\n",
       "20  bin_4_idx  0.000000\n",
       "21  nom_0_idx  0.000000\n",
       "22  nom_4_idx  0.000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "feature_imp = DataFrame( sorted(list(zip(pipelineModel.stages[-1].getInputCols(), \n",
    "                gbtModel.bestModel.featureImportances)), key = lambda t: t[1], reverse =True),\n",
    "                       columns = ['feature', 'weight'])\n",
    "\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'submission3.csv' file was scored by Kaggle giving the value of the area under ROC curve metric equal to 0.63203 on the test set. This result is not very high, given that the standard implementation of the xgboost classifier resulted in the value of 0.77 for the same metric.\n",
    "\n",
    "The purpose of this project was not to achieve the best score by using PySpark on a single machine, but rather to try a variety of methods built in the PySpark ML library which is more suitable for very large datasets trained on a cluster. We have convinced ourselves that the Gradient Boosting Tree classifier is probably the best one among the tree-based methods available in the pyspark.ml library, and that concomitant imputation of missing values is very convenient while performing fitting and transforming steps of the whole machine learning pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
